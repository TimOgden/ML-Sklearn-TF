{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances?\n",
    "\n",
    "I am not quite sure that the depth will reach 1 million, but I know it will get extremely large. It will overfit the training instances until it reaches 0 Gini impurity, and will not generalize well at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Is a node's Gini impurity generally lower or greater than its parent's? Is it *generally* lower/greater or *always* lower/greater?\n",
    "\n",
    "Gini impurity is lower in a child node than in its parent node, because the tree found an optimal threshold that will lower Gini impurity. This is always the case, because the CART algorithm \"stops recursing once it reaches the maximum depth... or it cannot find a split that will reduce impurity.\" If the algorithm cannot find a suitable solution, that node will not split further and will stay as a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?\n",
    "Yes. The `max_depth` in many ways functions as the number of parameters of the model, as in the case of Polynomial Regression/Classification. Therefore, reducing `max_depth` can prevent overfitting, but there are also other hyperparameters to modify as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "\n",
    "No, Decision Trees aren't influenced by differently scaled data. It's one of the benefits of using Decision Trees, in that you don't need to preprocess your data much at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. If it takes one hour to train a Decision Tree on a training set containing 1 million instances, roughly how much time will it take another Decision Tree on a training set containing 10 million instances?\n",
    "\n",
    "The training algorithm has a complexity of O(n * m log(m)). n is constant so we can ignore it, so the complexity of training 1mil instances is 1e6 * log(1e6) = 6,000,000, and the complexity of training 10mil instances is 1e7 * log(1e7) = 70,000,000.\n",
    "\n",
    "70,000,000 / 6,000,000 = 11.66667\n",
    "\n",
    "So the training of 10 million instances will take 11.6667 times longer, so 11.67 hours. Yikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. If your training set contains 100,000 instances, will setting `presort=True` speed up training?\n",
    "\n",
    "\"For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set `presort=True`), but this slows down training considerably for larger training sets.\"\n",
    "\n",
    "No, it will not speed it up, and in fact, will make it worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Train and find-tune a Decision Tree for the moons dataset.\n",
    "\n",
    "a. Generate a moons dataset using `make_moons(n_samples=10000, noise=0.4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "x, y = make_moons(n_samples=10000, noise=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Split into a training set and a test set using `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use grid search with cross-validation (with the help of the `GridSearchCV` class) to find good hyperparameter values for a `DecisionTreeClassifier`. Hint: try various values for `max_leaf_nodes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
