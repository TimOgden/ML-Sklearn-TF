{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "A lot of the time, your machine learning problems will involve thousands or millions of features (a 256x256 RGB image alone will have 196,608 features!). This will slow training down and also may hinder the algorithm's ability to generalize. This is commonly known as the **curse of dimensionality**.\n",
    "\n",
    "We do have methods of dimensionality reduction, however, which will speed up training by a lot. Of course, though, lossless compression is not a thing, so some information will be lost. In some cases, though, this could result in a better performance, actually.\n",
    "\n",
    "\"Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization... Reducing the number of dimensions down to two (or three) makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters.\"\n",
    "\n",
    "\n",
    "The issue with a high-dimensional object is that it behaves very differently than what we can think of. If you take a 10,000 dimensional unit hypercube and take a random point, there is a 99.9999% chance that that point will be located really, really close to the border. You can analogize this by thinking that if you consider enough \"dimensions\" about someone (e.g., how much sugar they put in their coffee, how many shirts they own, etc.), you could find a lot of examples of people lying in the extremes of those dimensions.\n",
    "\n",
    "\"Here is a more troublesome difference: if you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52.\" However, if you pick two random points in a 1,000,000-dimensional hypercube, the average distance between the two points will be around 408.25! \"This fact implies that high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other... In short, the more dimensions the training set has, the greater the risk of overfitting it.\"\n",
    "\n",
    "\"In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features (much less than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions.\"\n",
    "\n",
    "# Main Approaches for Dimensionality Reduction\n",
    "\n",
    "\"Let's take a look at the two main approaches to reducing dimensionality: projection and Manifold Learning.\"\n",
    "\n",
    "### Projection\n",
    "\n",
    "The concept of projection is actually really simple. \"In most real-world problems, training instances are *not* spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.\"\n",
    "\n",
    "In terms of 3D, this is as simple as finding a 2D plane that closely fits most of the dataset, and then doing a projection onto that plane. Bam, 3D just became 2D!\n",
    "\n",
    "### Manifold Learning\n",
    "\n",
    "In the case of the toy \"Swiss Roll\" dataset, a projection would simply squash the layers of the swiss roll and then the data is no longer linearly separable. \"The Swiss roll is an example of a 2D **manifold**. Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a d-dimensional manifold is a part of a n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.\"\n",
    "\n",
    "\"Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called **Manifold Learning**. It relies on the **manifold assumption**, ... which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifodl. This assumption is very often empirically observed.\"\n",
    "\n",
    "\"Hopefully, you now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms.\"\n",
    "\n",
    "### PCA\n",
    "\n",
    "**Principal Component Analysis (PCA)** is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
    "\n",
    "#### Preserving the Variance\n",
    "\n",
    "\"Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane.\" If you have a simple 2D dataset, the hyperplane is going to be a simple line. The line that fits best and preserves the most variance will be the one that is most \"parallel\" to the data. \"Another way to justify this... is that it... minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA.\"\n",
    "\n",
    "#### Principle Components\n",
    "\n",
    "\"PCA identifies the axis that accounts for the largest amount of variance in the training set... It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In [a] 2D example, there is no choice: it is the dotted line. If it were a higher-dimensional data-set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on-- as many axes as the number of dimensions in the dataset.\"\n",
    "\n",
    "\"The unit vector that defines the *i*th axis is called the *i*th **principal component** (PC).\" To find the prinicipal components of a training set, you can simply use the **Singular Value Decomposition** (SVD) standard matrix factorization technique. In Numpy, simply use the linalg.svd() function.\n",
    "\n",
    "**Note:** \"PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learn's PCA classes take care of centering the data for you. However, if you implement PCA yourself, ... don't forget to center the data first.\"\n",
    "\n",
    "#### Projecting Down to *d* Dimensions\n",
    "\n",
    "\"Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to *d* dimensions by projecting it onto the hyperplane defined by the first *d* principal components. Slecting this hyperplane ensures that the projection will preserve as much variance as possible.\"\n",
    "\n",
    "\"To project the training set onto the hyperplane, you can simply compute the matrix multiplication of the training set matrix X by the matrix Wd, defined as ... the matrix composed of the first *d* elements of V.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_centered = x - x.mean(axis=0) # centering data about origin\n",
    "U, s, Vt = np.linalg.svd(x_centered)\n",
    "W2 = Vt.T[:,:2] # w2 is the matrix with the first 2 columns of the V matrix.\n",
    "x2d = x_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x2D = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Scikit-Learn handles data centering about the origin for us.\n",
    "\n",
    "\"After fitting the PCA transformer to the dataset, you can access the principal components using the `components_` variable.\"\n",
    "\n",
    "#### The Explained Variance Ratio\n",
    "\n",
    "\"Another very useful piece of information is the **explained variance ratio** of each principal component, available via the `explained_variance_ratio_` variable. It indicates the proportion of the dataset's variance that lies along the axis of each principal component.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the Right Number of Dimensions\n",
    "\n",
    "\"Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for data visualization-- in that case you will generally want to reduce the dimensionality down to a 2 or 3.\"\n",
    "\n",
    "\"The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set's variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b011a56f676e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcumsum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(x_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum>=0.95) + 1 # + 1 is because indicies start at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You could then set `n_components=d` and run PCA again. However, there is a much better option. instead of specifying the number of principal components you want to preserve, you can set `n_components` to be a float between 0 and 1, indicating the ratio of variance you wish to preserve:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "x_reduced = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot `cumsum`). There will usually be an elbow in the curve, where the explained variance stops growing fast. You can think of this as the intrinsic dimensionality of the dataset.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
