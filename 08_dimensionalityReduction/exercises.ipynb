{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What are the main motivations for reducing a dataset's dimensionality? What are the main drawbacks?**\n",
    "\n",
    "\"Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization.\" As humans that live in a universe composed of 3 spatial dimensions, we have a lot of difficulty visualizing data that goes beyond the 3 dimensions. For example, something like an MNIST image will have 28\\*28=784 dimensions which can't be plotted on a graph with just two or three axes. Reducing dimensionality will both speed up the training process and also make it possible (in most cases) to plot your data and make it readable to humans.\n",
    "\n",
    "As for drawbacks, it's impossible to reduce a dataset's dimensionality without some loss, and thus will \"make your system perform slightly worse.\" The book also makes the case that \"it also makes your pipelines a bit more complex and thus harder to maintain,\" and while I think it's a valid point, I can't imagine that being anything beyond fairly trivial work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is the curse of dimensionality? **\n",
    "The curse of dimensionality is the accumulation of all the issues that arise from dealing with high-dimensional data. The inability to visualize or conceptualize, but also the fact that \"high-dimensional datasets are at risk of being very sparse.\" This means that two instances of data are much more likely to be far away from eachother than in a low-dimensional dataset. This will increase the likelihood of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?**\n",
    "\n",
    "Theoretically, yes. In the kPCA algorithm example, we used the inverse transform function to reconstruct the dataset and evaluate the loss function compared to the original dataset. However, there will be a lot of loss, and the reconstruction will be nowhere near perfect.\n",
    "\n",
    "Actually, ANN autoencoders can do this much better, because the whole point is to backpropogate to get the network to emulate the identity function as best as it can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?**\n",
    "\n",
    "Regular PCA is going to work linearly, so no. However, Kernel PCA works really well for high-dimensional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
