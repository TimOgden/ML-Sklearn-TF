{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this chapter, we will discuss RNN's that \"can analyze time series data\" and more generally can \"work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the nets we have discussed so far.\"\n",
    "\n",
    "\"In this chapter, we will look at the fundamental concepts underlying RNNs, the main problem they face (namely, vanishing/exploding gradients), and the solutions widely used to fight it: LSTM and GRU cells. Along the way, as always, we will show how to implement RNNs using TensorFlow. Finally, we will take a look at the architecture of a machine translation system.\"\n",
    "\n",
    "### Recurrent Neurons\n",
    "\n",
    "\"Up to now, we have mostly looked at feedforward neural networks, where the activations flow only in one direction, from the input layer to the output layer. A recurrent neural network loos very much like a feedforward neural network, except it also has connections pointing backward. Let's look at the simplest possible RNN, composed of just one neuron receiving inputs, producing an output, and sending that output back to itself.\"\n",
    "\n",
    "\"At each time step t, this recurrent neuron receives the inputs **x**(t) as well as its own output from the previous time step y(t-1).\"\n",
    "\n",
    "Now, instead of inputting the weights (transposed) times the inputs + the bias, we have\n",
    "\n",
    "activation( wx_t x **x** + wy_t x **y**(t-1) + b) where we have two separate weight vectors, wx and wy for the inputs and the outputs of the previous step.\n",
    "\n",
    "### Memory Cells\n",
    "\n",
    "\"Since the output of a recurrent neuron at time step t is a function of all the inputs from previous time steps, you could say that it has a form of *memory*. A part of a neural network that preserves some state across time steps is called a *memory cell* (or simply a *cell*). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell, but later in this chapter we will look at some more complex and powerful types of cells.\"\n",
    "\n",
    "### Input and Output Sequences\n",
    "\n",
    "There are 4 possible approaches that RNNs can take in terms of their inputs and outputs.\n",
    "\n",
    "- **Sequence-Sequence**: i.e., you input a sequence of data into the net, and it outputs another sequence. \"This is useful for predicting time series such as stock prices: you feed it the prices over the last N days, and it must output the prices shifted by one day into the future (i.e., from N-1 days ago to tomorrow).\n",
    "\n",
    "- **Sequence-Vector**: the idea here is that you input a sequence and then simply ignore all the outputs except the last one. This will give you one vector (or scalar if it's only one output neuron). \"For example: you could feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score.\"\n",
    "\n",
    "- **Vector-Sequence**: \"You could feed the network a single input at the first time step (and zeros for all other time steps), and let it output a sequence... For example, the input could be an image, and the output could be a caption for that image.\"\n",
    "\n",
    "- **Encoder-Decoder**: This approach actually combines a sequence-to-vector encoder followed by a vector-to-sequence decoder. \"For example, this can be used for translating a sentence from one language to another. You would feed the network a sentence in one language, the encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language. This two-step model... works much better than trying to translate on the fly with a single sequence-to-sequence RNN... since the last words of a sentence can affect the first words of the translation, so you need to wait until you have heard the whole sentence before translating it.\"\n",
    "\n",
    "![inputoutput](./inputoutput.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 150)               26850     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1510      \n",
      "=================================================================\n",
      "Total params: 28,360\n",
      "Trainable params: 28,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input((28, 28))\n",
    "rec = tf.keras.layers.SimpleRNN(150, activation='relu')(input_layer)\n",
    "dense = tf.keras.layers.Dense(10, activation='softmax')(rec)\n",
    "rnn = tf.keras.models.Model(inputs=input_layer, outputs=dense)\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4097 - accuracy: 0.8704 - val_loss: 0.2187 - val_accuracy: 0.9325\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1953 - accuracy: 0.9438 - val_loss: 0.1520 - val_accuracy: 0.9591\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1559 - accuracy: 0.9560 - val_loss: 0.1611 - val_accuracy: 0.9535\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1447 - accuracy: 0.9595 - val_loss: 0.1249 - val_accuracy: 0.9629\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1270 - accuracy: 0.9641 - val_loss: 0.1035 - val_accuracy: 0.9699\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1194 - accuracy: 0.9668 - val_loss: 0.1252 - val_accuracy: 0.9634\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1095 - accuracy: 0.9704 - val_loss: 0.1517 - val_accuracy: 0.9588\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1055 - accuracy: 0.9703 - val_loss: 0.1111 - val_accuracy: 0.9699\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0996 - accuracy: 0.9715 - val_loss: 0.0970 - val_accuracy: 0.9730\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0978 - accuracy: 0.9729 - val_loss: 0.0908 - val_accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d257c10d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training to Predict Time Series\n",
    "\n",
    "\"Each training instance is a randomly selected sequence of 20 consecutive values from the time series, and the target sequence is the same as the input sequence, except it is shifted by one time step into the future.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.3857803980708031,\n",
       " 0.7584261902904045,\n",
       " 1.1052637379071897,\n",
       " 1.4145251384022537]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use the function f(t) = sin(t)/3 + 2*sin(5t) for our time series.\n",
    "num_values = 800\n",
    "timeSeries = [math.sin(t)/3 + 2*math.sin(5*t) for t in np.linspace(0,30,num=num_values)]\n",
    "timeSeries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 20)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = [], []\n",
    "for i in range(int(num_values/n_steps)-1):\n",
    "    x_batch = timeSeries[n_steps*i:n_steps*(i+1)]\n",
    "    y_batch = timeSeries[n_steps*i+1:n_steps*(i+1)+1]\n",
    "    x.append(x_batch)\n",
    "    y.append(y_batch)\n",
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"At each timestep, we now have an output vector of size 100. But what we actually want is a single output value at each time step.\" We can solve this by adding a Dense layer of 1 neuron with no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_17 (SimpleRNN)    (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,301\n",
      "Trainable params: 10,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = tf.keras.models.Sequential()\n",
    "rnn.add(tf.keras.layers.SimpleRNN(n_neurons, activation='relu', input_shape=(20, 1)))\n",
    "rnn.add(tf.keras.layers.Dense(1, activation=None))\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train).reshape(-1, 20, 1)\n",
    "x_test = np.array(x_test).reshape(-1, 20, 1)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test).reshape(-1,20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 20, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.5257 - val_loss: 1.6260\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.5320 - val_loss: 1.6255\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.5303 - val_loss: 1.6417\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.5277 - val_loss: 1.6436\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.5204 - val_loss: 1.6233\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.5053 - val_loss: 1.6121\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4965 - val_loss: 1.6113\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4924 - val_loss: 1.6118\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.4905 - val_loss: 1.6131\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4936 - val_loss: 1.6143\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.4964 - val_loss: 1.6150\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4948 - val_loss: 1.6175\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.4936 - val_loss: 1.6174\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4929 - val_loss: 1.6137\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.4910 - val_loss: 1.6125\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.4914 - val_loss: 1.6138\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.4923 - val_loss: 1.6139\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.4915 - val_loss: 1.6138\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.4911 - val_loss: 1.6126\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.4896 - val_loss: 1.6106\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.4871 - val_loss: 1.6098\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.4863 - val_loss: 1.6092\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.4857 - val_loss: 1.6094\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4853 - val_loss: 1.6106\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.4858 - val_loss: 1.6110\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.4860 - val_loss: 1.6103\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.4858 - val_loss: 1.6098\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.4858 - val_loss: 1.6091\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.4854 - val_loss: 1.6084\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.4847 - val_loss: 1.6078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d2618e3370>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(x_train, y_train, epochs=30, validation_data=(x_test,y_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Difficulty of Training Over Many Time Steps\n",
    "\n",
    "As you increase the number of time steps of your network, the unrolled RNN will get enormous and super deep. This of course will lead to the vanishing/exploding gradients problem that we had before. You can still apply all the tricks that we discussed before: non-saturating activation functions, good parameter initialization, batch normalization, etc., but it still may not be enough and your network could take forever to train.\n",
    "\n",
    "\"The simplest and most common solution to this problem is to unroll the RNN only over a limited number of time steps during training. This is called **truncated backpropogation through time**... for example, in the time series prediction problem, you would simply reduce *n_steps* during training.\"\n",
    "\n",
    "\"A second problem faced by long-running RNNs is the fact that the memory of the first inputs gradually fades away... To solve this problem, various types of cells with long-term memory have been introduced.\"\n",
    "\n",
    "### LSTM Cell\n",
    "\n",
    "LSTM stands for *long short-term memory*. The LSTM cell was first proposed in 1997, and \"if you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect long-term dependencies in the data.\"\n",
    "\n",
    "![LSTM Cell](./lstm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state **c**(t-1) traverses the network from left to right, you can see that it first goes through a *forget gate*, dropping some memories, and then it adds some new memories via the addition operation (which adds the memories that were selected by an *input gate*). The result **c**(t) is sent straight out, without any further transformation. So at each time step, some memories are dropped and some memories are added. Moreover, after the addition operation, the long-term state is copied and passed through the tanh function, and then the result is filtered by the *output gate*. This produces the short-ter, state **h**(t) (which is equal to the cell's output for this timestep **y**(t)).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you can see, there are 4 fully connected layers, **f**, **g**, **i**, and **o**. Let's go over what these *gates* are for.\n",
    "\n",
    "- **g** is the basic logic. If this were an ordinary basic cell, this would be the whole operation. It takes in the inputs **x**(t) and the previous output **h**(t-1), passes it through a fully connected layer, and outputs **h**(t).\n",
    "\n",
    "- **f** is the *forget gate*. It takes **x**(t), **h**(t-1) and has a logistic function, thus it will output a vector of approximately 0's and 1's, and then this is elementwise multiplied by the long-term memory **c**(t-1) to forget some memories and keep some others.\n",
    "\n",
    "- **i** is the *input gate*. It takes the same inputs, applies a logistic function, and the multiplies it by the output of **g**(t), thus determining which parts of **g** will be added to the long-term memory.\n",
    "\n",
    "- **o** is the *output gate*. This finally determines what parts of the long-term meory that are going to be outputted, both as **h**(t) and **y**(t).\n",
    "\n",
    "\"In short, an LSTM cell can learn to recognize an important input (that's the role of the input gate), store it in the long-term state, learn to preserve it for as long as it is needed (that's the role of the forget gate), and learn to extract it whenever it is needed. This explains why they have been amazingly successful at capturing long-term patterns in time series, long texts, audio recordings, and more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
