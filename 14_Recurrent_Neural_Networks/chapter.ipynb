{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this chapter, we will discuss RNN's that \"can analyze time series data\" and more generally can \"work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the nets we have discussed so far.\"\n",
    "\n",
    "\"In this chapter, we will look at the fundamental concepts underlying RNNs, the main problem they face (namely, vanishing/exploding gradients), and the solutions widely used to fight it: LSTM and GRU cells. Along the way, as always, we will show how to implement RNNs using TensorFlow. Finally, we will take a look at the architecture of a machine translation system.\"\n",
    "\n",
    "### Recurrent Neurons\n",
    "\n",
    "\"Up to now, we have mostly looked at feedforward neural networks, where the activations flow only in one direction, from the input layer to the output layer. A recurrent neural network loos very much like a feedforward neural network, except it also has connections pointing backward. Let's look at the simplest possible RNN, composed of just one neuron receiving inputs, producing an output, and sending that output back to itself.\"\n",
    "\n",
    "\"At each time step t, this recurrent neuron receives the inputs **x**(t) as well as its own output from the previous time step y(t-1).\"\n",
    "\n",
    "Now, instead of inputting the weights (transposed) times the inputs + the bias, we have\n",
    "\n",
    "activation( wx_t x **x** + wy_t x **y**(t-1) + b) where we have two separate weight vectors, wx and wy for the inputs and the outputs of the previous step.\n",
    "\n",
    "### Memory Cells\n",
    "\n",
    "\"Since the output of a recurrent neuron at time step t is a function of all the inputs from previous time steps, you could say that it has a form of *memory*. A part of a neural network that preserves some state across time steps is called a *memory cell* (or simply a *cell*). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell, but later in this chapter we will look at some more complex and powerful types of cells.\"\n",
    "\n",
    "### Input and Output Sequences\n",
    "\n",
    "There are 4 possible approaches that RNNs can take in terms of their inputs and outputs.\n",
    "\n",
    "- **Sequence-Sequence**: i.e., you input a sequence of data into the net, and it outputs another sequence. \"This is useful for predicting time series such as stock prices: you feed it the prices over the last N days, and it must output the prices shifted by one day into the future (i.e., from N-1 days ago to tomorrow).\n",
    "\n",
    "- **Sequence-Vector**: the idea here is that you input a sequence and then simply ignore all the outputs except the last one. This will give you one vector (or scalar if it's only one output neuron). \"For example: you could feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score.\"\n",
    "\n",
    "- **Vector-Sequence**: \"You could feed the network a single input at the first time step (and zeros for all other time steps), and let it output a sequence... For example, the input could be an image, and the output could be a caption for that image.\"\n",
    "\n",
    "- **Encoder-Decoder**: This approach actually combines a sequence-to-vector encoder followed by a vector-to-sequence decoder. \"For example, this can be used for translating a sentence from one language to another. You would feed the network a sentence in one language, the encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language. This two-step model... works much better than trying to translate on the fly with a single sequence-to-sequence RNN... since the last words of a sentence can affect the first words of the translation, so you need to wait until you have heard the whole sentence before translating it.\"\n",
    "\n",
    "![inputoutput](./inputoutput.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 150)               26850     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1510      \n",
      "=================================================================\n",
      "Total params: 28,360\n",
      "Trainable params: 28,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.layers.Input((28, 28))\n",
    "rec = tf.keras.layers.SimpleRNN(150, activation='relu')(input_layer)\n",
    "dense = tf.keras.layers.Dense(10, activation='softmax')(rec)\n",
    "rnn = tf.keras.models.Model(inputs=input_layer, outputs=dense)\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train/255., x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4097 - accuracy: 0.8704 - val_loss: 0.2187 - val_accuracy: 0.9325\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1953 - accuracy: 0.9438 - val_loss: 0.1520 - val_accuracy: 0.9591\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1559 - accuracy: 0.9560 - val_loss: 0.1611 - val_accuracy: 0.9535\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1447 - accuracy: 0.9595 - val_loss: 0.1249 - val_accuracy: 0.9629\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1270 - accuracy: 0.9641 - val_loss: 0.1035 - val_accuracy: 0.9699\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1194 - accuracy: 0.9668 - val_loss: 0.1252 - val_accuracy: 0.9634\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1095 - accuracy: 0.9704 - val_loss: 0.1517 - val_accuracy: 0.9588\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.1055 - accuracy: 0.9703 - val_loss: 0.1111 - val_accuracy: 0.9699\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0996 - accuracy: 0.9715 - val_loss: 0.0970 - val_accuracy: 0.9730\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0978 - accuracy: 0.9729 - val_loss: 0.0908 - val_accuracy: 0.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d257c10d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training to Predict Time Series\n",
    "\n",
    "\"Each training instance is a randomly selected sequence of 20 consecutive values from the time series, and the target sequence is the same as the input sequence, except it is shifted by one time step into the future.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.3857803980708031,\n",
       " 0.7584261902904045,\n",
       " 1.1052637379071897,\n",
       " 1.4145251384022537]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use the function f(t) = sin(t)/3 + 2*sin(5t) for our time series.\n",
    "num_values = 800\n",
    "timeSeries = [math.sin(t)/3 + 2*math.sin(5*t) for t in np.linspace(0,30,num=num_values)]\n",
    "timeSeries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 20\n",
    "n_inputs = 1\n",
    "n_neurons = 100\n",
    "n_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = [], []\n",
    "for i in range(int(num_values/n_steps)):\n",
    "    x_batch = timeSeries[n_steps*i:n_steps*(i+1)]\n",
    "    y_batch = timeSeries[n_steps*i+1:n_steps*(i+1)+1]\n",
    "    #print(np.array(y_batch).shape)\n",
    "    x.append(x_batch)\n",
    "    y.append(y_batch)\n",
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"At each timestep, we now have an output vector of size 100. But what we actually want is a single output value at each time step.\" We can solve this by adding a Dense layer of 1 neuron with no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_12 (SimpleRNN)    (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 10,301\n",
      "Trainable params: 10,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn = tf.keras.models.Sequential()\n",
    "rnn.add(tf.keras.layers.SimpleRNN(n_neurons, activation='relu', input_shape=(20, 1)))\n",
    "rnn.add(tf.keras.layers.Dense(1, activation=None))\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train).reshape(-1, 20, 1)\n",
    "x_test = np.array(x_test).reshape(-1, 20, 1)\n",
    "y_train = np.array(y_train)\n",
    "y_train.shape\n",
    "#y_test = np.array(y_test).reshape(-1,20,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 20, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.9468 - val_loss: 2.0459\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.9341 - val_loss: 2.0167\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.9284 - val_loss: 2.0005\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.9275 - val_loss: 1.9980\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.9270 - val_loss: 2.0020\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.9258 - val_loss: 2.0100\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.9243 - val_loss: 2.0207\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.9229 - val_loss: 2.0345\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.9225 - val_loss: 2.0498\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.9227 - val_loss: 2.0628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d25d0b95b0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(x_train, y_train, epochs=10, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
