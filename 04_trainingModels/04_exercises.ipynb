{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "##### 1. What Linear Regression training algorithm can you use if you have a training set with millions of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Batch GD does scale well with a large number of features, but if you have millions of features, it's possible that, even with a small number of instances, this takes up too much memory. Therefore, Stochastic Gradient Descent would be most ideal and take up the least amount of memory.\n",
    "\n",
    "The Normal Equation would be the absolute worst option, because if you are trying to take the inverse of X_transposed \\* X and X is m by n where n is >1,000,000, there is absolutely no way that your memory can handle that calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any form of Gradient Descent is going to suffer by differently scaled features in your dataset, because the algorithm will quickly converge in any dimension that has a small scale, and then slowly meander its way towards the minimum in the elongated scale. Of course though, it will still reach the same minimum (assuming its convex), it just may take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope. The *Logistic Regression cost function (log loss)* is convex. That means there is only one minimum, so if the GD algorithm reached a minimum, you can be certain that it is the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, they don't. Batch Gradient Descent will, since it uses the whole training set to calculate the gradient and move in its opposite direction. Therefore, it will reach the exact point of minimal cost, assuming we are talking about the minimal cost of the training set, not the impossible to calculate \"ideal\" cost of the whole population.\n",
    "\n",
    "Unlike Batch, Stochastic and Mini-batch have an element of randomness to them. They only use a sample of the training set to calculate the gradient, and therefore will have some error in its calculation from the actual true gradient. With this impercise measurement, the algorithm will continue hopping around close to the true minimum, but will likely give you a different result from Batch GD and in fact will likely give you a slightly different result every time you train the same Stochastic or Mini-batch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistenly goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is likely overfitting. As validation error rises, if training error decreases, that is a telltale sign of overfitting. To prevent this, add more training instances, or implement regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
