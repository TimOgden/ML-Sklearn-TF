{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Draw an ANN using the original artifical neurons that comput A XOR B. Hint: A XOR B = (A and !B) or (!A and B)**\n",
    "\n",
    "![XOR](./XOR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?**\n",
    "\n",
    "The classical Perceptron uses the step function as its activation. Thus, it will only return a 0 if it doesn't think a certain class applies and 1 if it does. Logistic Regression on the other hand gives a probability value for each class. To fix this, we just need to use the sigmoid function as the perceptron's activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Why was the logistic activation function a key ingredient in training the first MLPs?**\n",
    "\n",
    "If you were only using the step activation function in your MLP, you would have a non-differentiable point at x=0 and dy/dx = 0 everywhere else. When you are computing the gradient at each step, this is not helpful and gives no information on the \"direction to head downhill\" to minimize the loss. The logistic activation function gives a nice derivative that points in the direction of minimizing loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Name three popular activation functions. Can you draw them?**\n",
    "\n",
    "![Activation functions](./activations.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finall one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.**\n",
    "\n",
    "\n",
    "- **What is the shape of the input matrix X?**\n",
    "\n",
    "The shape would be (batch size, 10) because there are *batch size* instances each with 10 features.\n",
    "\n",
    "- **What about the shape of the hidden layer's weight vector Wh and the shape of its bias vector bh?**\n",
    "\n",
    "The weight vector has shape (50) for each of the 50 neurons and the bias vector has shape (1) because each layer has one bias value.\n",
    "\n",
    "- **What is the shape of the output layer's weight vector Wo and its bias vector bo?**\n",
    "\n",
    "The weight vector has shape (3) because there are 3 output neurons and the bias vector has no shape because the output has no bias value.\n",
    "\n",
    "- **What is the shape of the network's output matrix Y?**\n",
    "\n",
    "It is (batch size, 3).\n",
    "\n",
    "- **Write the equation that computes the network's output matrix Y as a function of X, Wh, bh, Wo, bo.**\n",
    "\n",
    "**Y** = (**X**_transposed x Wh)_transposed x Wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? In instead you wanted to tackle MNIST, how many neurons do you need in the output layer, using what activation function? Answer the same questions for getting your network to predict housing prices as in Chapter 2.**\n",
    "\n",
    "\n",
    "- Email: 2 output neurons with softmax\n",
    "\n",
    "- MNIST: 10 output neurons with softmax\n",
    "\n",
    "- Housing prices: 1 output neuron with no activation fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What is backpropagation and how does it work? What is the difference betwen backpropogation and reverse-mode autodiff?**\n",
    "\n",
    "Backpropogation is the process of computing the error contributions of a hidden layer on the next layer, and then repeating the same process on the previous hidden layer, ie computing the contributions of the previous layer on the previously computed error contributions. This process repeats for every hidden layer in the network. It is no different from reverse-mode autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?**\n",
    "\n",
    "You can reduce the number of neurons in each layer as that will reduce the complexity the model can understand. You could reduce the number of layers which is likely the best way to reduce overfitting. Another hyperparameter would be activation functions but changing these shouldn't have much effect on overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Just like in the last exercise of Chapter 9, try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, plot learning curves using TensorBoard, and so on).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(150, activation='relu'),\n",
    "    tf.keras.layers.Dropout(.33),\n",
    "    tf.keras.layers.Dense(150, activation='relu'),\n",
    "    tf.keras.layers.Dropout(.33),\n",
    "    tf.keras.layers.Dense(150, activation='relu'),\n",
    "    tf.keras.layers.Dropout(.33),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=2,restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0728 - accuracy: 0.9789\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0707 - accuracy: 0.9787\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0723 - accuracy: 0.9797\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0688 - accuracy: 0.9794\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0669 - accuracy: 0.9809\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0672 - accuracy: 0.9800\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0656 - accuracy: 0.9806\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0628 - accuracy: 0.9815\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0600 - accuracy: 0.9826\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0634 - accuracy: 0.9817\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0655 - accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b1028bf940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, callbacks=[early_stop],batch_size=32,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0814 - accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08138713985681534, 0.9814000129699707]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test) # Achieves an accuracy of 98.14% after 31 epochs of training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
