{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks\n",
    "\n",
    "\"In this chapter, we will introduce artificial neural networks, starting with a quick tour of the very first ANN architectures. Then, we will present *Multi-Layer Perceptrons* and implement one using TensorFlow to tackle the MNIST digit classification problem.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the biological world, \"individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions of neurons, each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a vast network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants.\n",
    "\n",
    "### Logical Computations with Neurons\n",
    "\n",
    "\"Warren McCulloch and Walter Pitts proposed a very simple model of the biological neuron, which later became known as an *artificial neuron*: it has one or more binary (on/off) inputs and one binary output. The artificial neuron simply activates its output when more than a certain number of its inputs are active. McCulloch and Pitts showed that even with such a simplified model, it is possible to build a network of artificial neurons that computes any logical proposition you want.\"\n",
    "\n",
    "![ANNs performing simple logical computations](./one.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "\n",
    "\"The perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron called a *threshold logic unit (TLU)*, or sometimes a linear threshold unit: the inputs and output are now numbers... and each input connection is associated with a weight. The TLU computs the weighted sum of its inputs (z = w1*x1 + w2*x2 + ... + wnxn = **w**^T times **x**), then applies a *step function* to that sum and outputs the result.\n",
    "\n",
    "![Threshold Logic Unit](./two.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A single TLU can be used for simple lineary binary classification. It computes a linear combination of the inputs and if the result exceeds a threshold, it outputs the positive class or else outputs the negative class.\" I.e., you could have a linearly seperable dataset with two dimensions (plus the addition of x0 = 1 for the bias term), and the TLU will compute the linear combination of the inputs and run that through the step function.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"So how is a Perceptron trained? The Perceptron training algorithm proposed by Frank Rosenblatt was largely inspired by *Hebb's rule*,\" which is the idea commonly paraphrased as \"cells that fire together, wire together\". The idea is that \"the connection weight between two neurons is increased whenever they have the same output. Perceptrons are trained using a variant of this rule that takes into account the error made by the network; it does not reinforce connections that lead to the wrong output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"The perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation\n",
    "\n",
    "We can create a multi layer perceptron by stacking layers of neurons and connecting every neuron in the previous layer to every neuron in the new layer. Multi-layer perceptrons or ***MLP's*** are much more capable of solving more complex problems, and are capable of classifying data that is not linearly seperable.\n",
    "\n",
    "How do we train these perceptrons though? The trick is to use **backpropogation**. \"Today we would describe it as Gradient Descent using reverse-mode autodiff.\"\n",
    "\n",
    "The idea here is to feed the neural network an instance and compute the output of every neuron in each layer. Now, we measure the error, i.e. abs(y - y_hat), \"and it computes how much each neuron in the last hidden layer contributed to each output neuron's error.\" Now you have the information of the error contributions of the last hidden layer for each neuron. Now, you just compute how much each neuron in the *second to last* hidden layer contributed to each of the error contributions of the last hidden layer.\n",
    "\n",
    "This is a recursive process that you can do on each hidden layer of the network going backwards until you hit the first. Now, you can just use Gradient Descent to take a small step down the gradient function and then update the weights accordingly.\n",
    "\n",
    "There is an issue here, though. The step function that we were using before in the TLU clearly doesn't have a helpful derivative. A solution is to instead use the sigmoid function, or of course we can use Tanh or ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Sklearn\n",
    "\n",
    "The easiest way to train a MLP is to use sklearn. \"The `MLPClassifier` class makes it fairly easy to train a deep neural network with any number of hidden layers and a softmax output layer to output estimated class probabilities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)] # petal length, petal width\n",
    "y = (iris.target==0).astype(int) # Iris Setosa?\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=50, hidden_layer_sizes=(300, 100), max_iter=20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(300,100,), activation='relu', max_iter=20, batch_size=50)\n",
    "dnn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = dnn_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a DNN Using Plain TensorFlow\n",
    "\n",
    "\"If you want more control over the architecture of the network, you may prefer to use Tensorflow's lower-level Python API.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(300, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2036 - accuracy: 0.9387\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0869 - accuracy: 0.9732\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0579 - accuracy: 0.9815\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0428 - accuracy: 0.9863\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0339 - accuracy: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x258eecc7760>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07510446012020111, 0.9794999957084656]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = x_test[:4]\n",
    "np.argmax(model.predict(sample), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
