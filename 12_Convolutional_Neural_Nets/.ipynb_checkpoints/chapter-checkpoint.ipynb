{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "CNN's are the most practical way to make visual-based neural networks. They also work well for other non-visual tasks such as  NLP and voice recognition, but for now we will just focus on the visual aspects.\n",
    "\n",
    "The idea of convolutional networks is based on how the human visual cortex works. Many neuroms in the visual cortext \"have a small *local receptive field*, meaning they react only to visual stimuli located in a limited region of the visual field.\" The neurons also work hierarchaly just like a regualar perceptron, and lower neurons will detect lines and edges of specific relations and the high level neurons will detect more complex shapes and patterns.\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "\"Neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields. In turn each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer.\"\n",
    "\n",
    "This process of creating partially connected layers is much better than Dense networks since it significantly reduces the exploding exponentiation of connections.\n",
    "\n",
    "\"Until now, all multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the neural network. Now each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs.\"\n",
    "\n",
    "We can change the size of the fields and also the *stride* or the jumps between each receptive fields. It is definitely possible that the receptive fields are too large at the edges that they don't fit entirely on the image. This is obviously true when the layers are the same size and shape. To remedy this, we can use padding, such as surrounding the border of the image with 0's or the same as the original border so that we have more room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters\n",
    "\n",
    "Since we now have 2-dimensional layers, we can consider a layer's weights as a grayscale image. We can consider several pre-built *filters* or *convolution kernels*, for example a horizontal filter, which is a 7x7 matrix with 0's everywhere except for a horizontal line of 1's in the middle. Another possible filter is the vertical filter, which you can imagine is similar but the 1's make a vertical line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "hor = np.zeros((7,7))\n",
    "hor[3] = np.ones(7)\n",
    "vert = np.zeros((7,7))\n",
    "vert[:,3] = 7\n",
    "plt.subplot(121)\n",
    "plt.title('Horizontal filter')\n",
    "plt.imshow(hor, cmap=plt.cm.gray)\n",
    "plt.subplot(122)\n",
    "plt.title('Vertical filter')\n",
    "plt.imshow(vert, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a layer that has all its neurons using the vertical filter, and we fed an image into it, all the vertical lines in the image will enhance all the vertical lines while blurring out the rest of the image. \"Thus, a layer full of neurons using the same filter gives you a *feature map*, wich highlights the areas in an image that are most similar to the filter. During training, a CNN finds the most useful filters for its task and it learns to combine them into more complex patterns.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Multiple Feature Maps\n",
    "\n",
    "\"Up to now, for simplicity, we have represented each convolutional layer as a thin 2D layer, but in reality it is composed of several feature maps of equal sizes, so it is more accurately represented in 3D. Within one feature map, all neurons share the same parameters (weights and bias term) [which is its filter] but different feature maps may have different parameters.\"\n",
    "\n",
    "\"The fact that all neurons in a feature map share the same paramter dramatically reduces the number of parameters in the model, but most importantly it means that once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Specifically, a neuron located in row i, column j of jthe feature map k in a given convolutional layer l is connected to the output of the neurons in the previous layer l-1 located in rows i x stride_h to i x stride_h + f_h - 1 and columns j x stride_w to j x stride_w + f_w - 1 across all feature maps in layer l-1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Requirements\n",
    "\n",
    "One issue with CNNs is that all the intermediate values that are computed during the forward pass need to be held in memory for the backpropogation pass.\n",
    "\n",
    "Now let's look at the second common building block of CNNs: the *pooling layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers are actually really simple. They work very similar in that each neuron has a receptive field that it is connected to in the next layer, and you can adjust these with the size, stride and controlling the padding, but the neuron actually has no weights at all. Instead, it simply works as an aggregate function. It will either find the maximum value of each value in the receptive field, or it will find the average value of each value in the receptive field.\n",
    "\n",
    "Pooling layers are useful in subsampling or shrinking \"the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architectures\n",
    "\n",
    "Usually, CNN architectures are structured as having a few Convolution layers stacked on top of each other with a ReLU activation and with pooling layers in between. \"The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps) thanks to the convolutional layers. Then once the image is small enough and you have enough feature maps, you can then add a few fully connected layers and a final layer to output a regression or classification depending on your task.\n",
    "\n",
    "However, very powerful alternate CNN architectures have come about in recent years that use very different approaches. We will summarize and try to implement (sometimes simplified) versions of the same architectures in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet-5\n",
    "\n",
    "\"The LeNet-5 architecture is perhaps the most widely known CNN architecture. As mentioned earlier, it was created by Yann LeCun in 1998 and widely used for hand-written digit recognition (MNIST).\"\n",
    "\n",
    "| Layer | Type            | Maps | Size  | Kernel size | Stride | Activation |\n",
    "|-------|-----------------|------|-------|-------------|--------|------------|\n",
    "| Out   | Fully Connected | -    | 10    | -           | -      | RBF        |\n",
    "| F6    | Fully Connected | -    | 84    | -           | -      | tanh       |\n",
    "| C5    | Convolution     | 120  | 1x1   | 5x5         | 1      | tanh       |\n",
    "| S4    | Avg Pooling     | 16   | 5x5   | 2x2         | 2      | tanh       |\n",
    "| C3    | Convolution     | 16   | 10x10 | 5x5         | 1      | tanh       |\n",
    "| S2    | Average Pooling | 6    | 14x14 | 2x2         | 2      | tanh       |\n",
    "| C1    | Convolution     | 6    | 28x28 | 5x5         | 1      | tanh       |\n",
    "| In    | Input           | 1    | 32x32 | -           | -      | -          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"There are a few extra details to be noted:\n",
    "\n",
    "- MNIST images are 28x28 pixels, but they are zero-padded to 32x32 pixels and normalized before being fed to the network. The rest of the network does not use any padding, which is why the size keeps shrinking as the image progresses through the network.\n",
    "\n",
    "- The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coefficient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function.\n",
    "\n",
    "- Most neurons in C3 maps are connected to neurons in only three or four S2 maps (instead of all six S2 maps). [Note: I don't know too much about that and I don't want to spend hours on this one network, so we will just disregard that.]\n",
    "\n",
    "- The output layer is a bit special: instead of computing the matrix multiplication of the inputs and the weight vector, each neuron outputs the square of the Euclidean distance between its input vector and its weight vector. Each output measures how much the image belongs to a particular digit class. The cross entropy cost function is now preferred, as it penalizes bad predictions much more, producing larger gradients and thus converging faster.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to define this slightly modified average pooling layer ourselves\n",
    "\n",
    "class AvgPooling2D_Linear(tf.keras.layers.AveragePooling2D):\n",
    "    \n",
    "    def __init__(self, input_dim=6, strides=2, activation=None):\n",
    "        super(AvgPooling2D_Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim,), dtype='float32'), trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(input_dim,), dtype='float32'), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also we need to define the modified output layer\n",
    "\n",
    "class Alternate_Output(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=32, inputs_dim=32):\n",
    "        super(Alternate_Output, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(inputs_dim, units),\n",
    "                                                  dtype='float32'), trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
    "                                                  dtype='float32'),  trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return tf.abs(tf.math.subtract(inputs,self.w)) # I don't understand how I can find the Euclidean distance\n",
    "                                                        # with 84 inputs and 10 weights??? Whatever, I'll just use a regular dense layer for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input((28,28,1))\n",
    "zero_pad = tf.keras.layers.ZeroPadding2D(padding=(2,2))(inputs)\n",
    "c1 = tf.keras.layers.Conv2D(6, (5,5), strides=1, activation='tanh')(zero_pad)\n",
    "s2 = AvgPooling2D_Linear(input_dim=6, strides=2, activation='tanh')(c1)\n",
    "c3 = tf.keras.layers.Conv2D(16, (5,5), strides=1, activation='tanh')(s2)\n",
    "s4 = AvgPooling2D_Linear(input_dim=16, strides=2, activation='tanh')(c3)\n",
    "c5 = tf.keras.layers.Conv2D(120, (5,5), strides=1, activation='tanh')(s4)\n",
    "f6 = tf.keras.layers.Dense(84, activation='tanh')(c5)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(f6)\n",
    "leNet = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
