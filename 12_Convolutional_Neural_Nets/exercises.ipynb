{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What are the advantages of a CNN over a fully connected DNN for image classification?**\n",
    "\n",
    "Convolutional layers have *partially connected* neurons, so in large images, the number of parameters is much more manageable. Also, in each feature map in a conv layer, all the neurons share the same weights and bias terms, meaning that a) there is way fewer parameters and b) if the network learns how to detect a certain feature, it will be able to detect that feature over the whole image, unlike a DNN where it has to learn to detect that same feature multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Consider a CNN composed of three convolutional layers, each with 3x3 kernels, a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200x300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?**\n",
    "\n",
    "The total number of parameters per layer would be (kernel_area x num_channels + 1 (for bias)) x num_filters. Thus, we have\n",
    "\n",
    "( (3x3x3+1)  x 100 ) + ( (3x3x3+1) x 200 ) + ( (3x3x3+1) x 400 ) = 2800 + 5600 + 11200 = 19,600 total parameters.\n",
    "\n",
    "If we are inferencing on a single instance, we only need at maximum, enough RAM to hold the largest two consecutive layers in memory. Since we have strides of 2 in all our layers, that means that the first conv has (100x150x100)=1,500,000 neurons, then the second would has (50x75x200)=3,000,000 neurons, then the last has (25x38x400)=6,000,000 neurons, and each neuron has 3^3=27 inputs, so when training on a mini-batch of 50 images, you would need 27x(1,500,000+3,000,000+6,000,000)x32x50 bits = 56,700 Mb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?**\n",
    "\n",
    "- Decrease the batch size\n",
    "\n",
    "- Use a stride to decrease dimensionality.\n",
    "\n",
    "- Use less layers.\n",
    "\n",
    "- Use a pooling layer.\n",
    "\n",
    "- Use 16-bit floats rather than 32-bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While convolutional layers with a stride > 1 will be able to output smaller images than their inputs, they still add a lot of complexity. There are parameters based on the kernel size, the number of channels, and the number of feature maps. A pooling layer will shrink the image the same way, but it doesn't have any weight values and thus is not trainable. Pooling layers are very useful for removing complexity from the network, and thus reducing the possibility of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. When would you want to add a local response normalization layer?**\n",
    "\n",
    "If you want to increase the generalizability of your network, adding local response normalization is a good idea because it will encourage each layer to find a range of feature maps that are very different from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Can you name the main innovations in AlexNet compared to LeNet-5? What about the main innovations in GoogLeNet and ResNet?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main innovation of AlexNet compared to LeNet-5 was the fact that it was much deeper than LeNet. It also stacked multiple Conv layers in succession, and AlexNet used local response normalization.\n",
    "\n",
    "The main innovation of ResNet over GoogLeNet was the introduction of skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Build your own CNN and try to achieve the highest possible accuracy on MNIST.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What I want to implement in this CNN\n",
    "# 1. Data augmentation\n",
    "# 2. Not too large, about the size of AlexNet\n",
    "# 3. Some regularization techniques\n",
    "# 4. *Maybe* implementing local response regularization\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = tf.keras.layers.Input((28,28,1))\n",
    "conv1 = tf.keras.layers.Conv2D(5, 3, strides=2, padding='SAME', \n",
    "                               activation=tf.keras.activations.elu,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=0.02))(img_input)\n",
    "batch_norm_a = tf.keras.layers.BatchNormalization()(conv1)\n",
    "pool2 = tf.keras.layers.MaxPool2D(2)(batch_norm_a)\n",
    "conv3 = tf.keras.layers.Conv2D(10, 3, strides=2, padding='SAME',\n",
    "                               activation=tf.keras.activations.elu,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=0.02))(pool2)\n",
    "batch_norm_b = tf.keras.layers.BatchNormalization()(conv3)\n",
    "flatten4 = tf.keras.layers.Flatten()(batch_norm_b)\n",
    "dense5 = tf.keras.layers.Dense(50, activation=tf.keras.activations.elu)(flatten4)\n",
    "dropout6 = tf.keras.layers.Dropout(.5)(dense5)\n",
    "dense7 = tf.keras.layers.Dense(10, activation='softmax')(dropout6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Model(inputs=[img_input], outputs=[dense7])\n",
    "cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 5)         50        \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 5)         20        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 5)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 10)          460       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 10)          40        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50)                8050      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 9,130\n",
      "Trainable params: 9,100\n",
      "Non-trainable params: 30\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((-1,28,28,1))\n",
    "x_test = x_test.reshape((-1,28,28,1));\n",
    "x_test = x_test/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    featurewise_std_normalization=True, featurewise_center=True,\n",
    "    rotation_range=20, shear_range=3,\n",
    "    rescale=1/255.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2,restore_best_weights=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "   2/1875 [..............................] - ETA: 13:08 - loss: 0.5963 - accuracy: 0.7812WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.429220). Check your callbacks.\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.4814 - accuracy: 0.8656 - val_loss: 1.0729 - val_accuracy: 0.6326\n",
      "Epoch 2/15\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.3933 - accuracy: 0.8922 - val_loss: 0.9133 - val_accuracy: 0.6911\n",
      "Epoch 3/15\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.3641 - accuracy: 0.9007 - val_loss: 1.0287 - val_accuracy: 0.6605\n",
      "Epoch 4/15\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3449 - accuracy: 0.9071 - val_loss: 0.8419 - val_accuracy: 0.7339\n",
      "Epoch 5/15\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.3300 - accuracy: 0.9104 - val_loss: 0.6152 - val_accuracy: 0.8476\n",
      "Epoch 6/15\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3252 - accuracy: 0.9121 - val_loss: 0.8907 - val_accuracy: 0.6910\n",
      "Epoch 7/15\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3155 - accuracy: 0.9160 - val_loss: 0.9463 - val_accuracy: 0.6728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1792e5aaa60>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(datagen.flow(x_train, y_train, batch_size=32), validation_data=(x_test, y_test), \n",
    "        callbacks=[early_stop, tensorboard], epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Transfer learning for large image classification.**\n",
    "\n",
    "*a. Load in the flowers dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
