{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He inititialization?**\n",
    "\n",
    "No, definitely not. The idea of selecting all your weights from the +/- sqrt(6/(n_inputs+n_outputs)) interval is so that you will have an approximately equal variance of the inputs and outputs of that layer. If you set all the weights in a layer to the same value, you will have variance 0 which is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Is it okay to initialize the bias terms to 0?**\n",
    "\n",
    "Yes, it will make no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Name three advantages of the ELU activation function over ReLU.**\n",
    "\n",
    "- ELU gives a nice, smooth gradient for negative z values while the ReLU has gradient 0 if z<0.\n",
    "\n",
    "- ELU has negative values when z<0, meaning that the average outputs will be close to 0. This will alleviate vanishing gradient problems.\n",
    "\n",
    "- \"Training time was reduced and the neural network performed better on the test set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. In which cases would you want to use each of the following activation functions?**\n",
    "\n",
    "- ELU: good for reducing the vanishing gradients problem\n",
    "\n",
    "- leaky ReLU: if you notice that a lot of your neurons are \"dying\", simply changing the activation to leaky_ReLU may help.\n",
    "\n",
    "- ReLU: the fastest of the options to compute, good if you want a very quick lightweight model.\n",
    "\n",
    "- tanh: same as logistic but goes between -1 and 1. Could be used as the output layer if you want a -1 definitely false, +1 definitely true kind of confidence scale.\n",
    "\n",
    "- logistic: Could be used as the output layer if you want a 0 no idea, +1 definitely true kind of confidence scale.\n",
    "\n",
    "- softmax: Used as the output layer in multi-class classification tasks. Chooses the highest probability score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using the SGD optimizer?**\n",
    "\n",
    "The algorithm will act almost like there is no friction and it may completely overshoot the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Name three ways you can produce a sparse model.**\n",
    "\n",
    "1. Set a threshold value and any weights that are below that value are set to 0.\n",
    "\n",
    "2. Use heavy l2 regularization to punish non-zero values.\n",
    "\n",
    "3. Implement Follow the Regularized Leader regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Does dropout slow down training? Does it slow dow inference?**\n",
    "\n",
    "Yes, dropout does slow down training, but it is typically worth the slowdown in terms of building a more robust model. It will not slow down inference however because dropout is not applied during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Deep Learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a. Build a DNN with 5 hidden layers of 100 neurons each, He initialization, and the ELU activation function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "early_stop = tf.keras.callbacks.EarlyStopping()\n",
    "tensorboard = tf.keras.callbacks.TensorBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.isin(y_train,np.arange(0,5))\n",
    "test_filter = np.isin(y_test, np.arange(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/957 [..............................] - ETA: 4:22 - loss: 1.4403 - accuracy: 0.4219WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286759). Check your callbacks.\n",
      "957/957 [==============================] - 3s 3ms/step - loss: 0.1163 - accuracy: 0.9655 - val_loss: 0.0981 - val_accuracy: 0.9665\n",
      "Epoch 2/20\n",
      "957/957 [==============================] - 2s 3ms/step - loss: 0.0604 - accuracy: 0.9818 - val_loss: 0.0460 - val_accuracy: 0.9852\n",
      "Epoch 3/20\n",
      "957/957 [==============================] - 2s 3ms/step - loss: 0.0431 - accuracy: 0.9866 - val_loss: 0.0491 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19982e572b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train[train_filter], y_train[train_filter], validation_data=(x_test[test_filter],y_test[test_filter]),\n",
    "         epochs=20, callbacks=[early_stop,tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*c. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/957 [..............................] - ETA: 15:15 - loss: 1.9768 - accuracy: 0.2969WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.966616). Check your callbacks.\n",
      "957/957 [==============================] - 6s 6ms/step - loss: 0.1224 - accuracy: 0.9634 - val_loss: 0.0395 - val_accuracy: 0.9887\n",
      "Epoch 2/20\n",
      "957/957 [==============================] - 3s 4ms/step - loss: 0.0690 - accuracy: 0.9790 - val_loss: 0.0427 - val_accuracy: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1998257e8b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train[train_filter], y_train[train_filter], validation_data=(x_test[test_filter],y_test[test_filter]),\n",
    "         epochs=20, callbacks=[early_stop,tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does converge a lot faster, even if each epoch is slightly longer. This added time will be less as the epochs go on though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_dropout = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer='glorot_uniform',activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "model2_dropout.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/957 [..............................] - ETA: 18:15 - loss: 2.4672 - accuracy: 0.2812WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.157248). Check your callbacks.\n",
      "957/957 [==============================] - 6s 7ms/step - loss: 0.4952 - accuracy: 0.8389 - val_loss: 0.0943 - val_accuracy: 0.9708\n",
      "Epoch 2/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.2316 - accuracy: 0.9332 - val_loss: 0.0787 - val_accuracy: 0.9755\n",
      "Epoch 3/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.1971 - accuracy: 0.9438 - val_loss: 0.0674 - val_accuracy: 0.9796\n",
      "Epoch 4/20\n",
      "957/957 [==============================] - 4s 5ms/step - loss: 0.1752 - accuracy: 0.9519 - val_loss: 0.0564 - val_accuracy: 0.9819\n",
      "Epoch 5/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.1585 - accuracy: 0.9569 - val_loss: 0.0546 - val_accuracy: 0.9829\n",
      "Epoch 6/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.1506 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 0.9844\n",
      "Epoch 7/20\n",
      "957/957 [==============================] - 5s 5ms/step - loss: 0.1303 - accuracy: 0.9633 - val_loss: 0.0439 - val_accuracy: 0.9858\n",
      "Epoch 8/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.1232 - accuracy: 0.9666 - val_loss: 0.0402 - val_accuracy: 0.9879\n",
      "Epoch 9/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.1190 - accuracy: 0.9684 - val_loss: 0.0402 - val_accuracy: 0.9883\n",
      "Epoch 10/20\n",
      "957/957 [==============================] - 4s 4ms/step - loss: 0.1157 - accuracy: 0.9691 - val_loss: 0.0451 - val_accuracy: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19982bdf640>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_dropout.fit(x_train[train_filter], y_train[train_filter], validation_data=(x_test[test_filter],y_test[test_filter]),\n",
    "         epochs=20, callbacks=[early_stop,tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Transfer learning**\n",
    "\n",
    "*a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them and replaces the softmax output layer with a new one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = tf.keras.models.Sequential()\n",
    "[transfer_model.add(layer) for layer in model2_dropout.layers[:-1]]\n",
    "for layer in transfer_model.layers:\n",
    "    layer.trainable=False\n",
    "transfer_model.add(tf.keras.layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b. Train the new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Can you achieve high precision?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255., x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.where(np.logical_and(y_train>=5,y_train<=9))\n",
    "test_filter = np.where(np.logical_and(y_test>=5,y_test<=9))\n",
    "y_train = y_train[train_filter]\n",
    "y_test = y_test[test_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train-5, num_classes=5)\n",
    "y_test = tf.keras.utils.to_categorical(y_test-5,num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " logits and labels must have the same first dimension, got logits shape [32,5] and labels shape [160]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-128-2a3e0a44182a>:1) ]] [Op:__inference_train_function_140761]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-2a3e0a44182a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m transfer_model.fit(x_train[:500],y_train[:500],\n\u001b[0m\u001b[0;32m      2\u001b[0m                    validation_data=(x_test,y_test), epochs=20, callbacks=[tensorboard,early_stop])\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  logits and labels must have the same first dimension, got logits shape [32,5] and labels shape [160]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at <ipython-input-128-2a3e0a44182a>:1) ]] [Op:__inference_train_function_140761]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "transfer_model.fit(x_train[:500],y_train[:500],\n",
    "                   validation_data=(x_test,y_test), epochs=20, callbacks=[tensorboard,early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 121,405\n",
      "Trainable params: 505\n",
      "Non-trainable params: 120,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Pretraining on an auxillary task.**\n",
    "\n",
    "*a. Build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_a = tf.keras.layers.Input((28,28),name='input_a')\n",
    "dense1 = tf.keras.layers.Dense(100, activation=tf.keras.activations.elu,name='hidden1_a')(input_a)\n",
    "dense2 = tf.keras.layers.Dense(100, activation=tf.keras.activations.elu,name='hidden2_a')(dense1)\n",
    "output_a = tf.keras.layers.Dense(100, activation=tf.keras.activations.elu,name='hidden3_a')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_b = tf.keras.layers.Input((28,28),name='input_b')\n",
    "dense1 = tf.keras.layers.Dense(100, activation=tf.keras.activations.elu,name='hidden1_b')(input_b)\n",
    "dense2 = tf.keras.layers.Dense(100, activation=tf.keras.activations.elu,name='hidden2_b')(dense1)\n",
    "output_b = tf.keras.layers.Dense(100, activation=tf.keras.activations.elu,name='hidden3_b')(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = tf.keras.layers.Concatenate(axis=1,name='concat')([output_a,output_b])\n",
    "dense_total = tf.keras.layers.Dense(100,activation=tf.keras.activations.elu,name='hidden_total')(concat)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=[input_a,input_b], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# Generate image of model\n",
    "tf.keras.utils.plot_model(model,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=[tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*b. Split the MNIST training set in two sets: split #1 should contain 55,000 images, and split #2 should contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000 5000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (_,_) = tf.keras.datasets.mnist.load_data()\n",
    "x_test, y_test = x_train[-5000:], y_train[-5000:]\n",
    "x_train, y_train = x_train[:-5000], y_train[:-5000]\n",
    "print(len(x_train),len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "organized = [x_train[np.where(y_train==val)] for val in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional or an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-dc92091ff7b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-164-dc92091ff7b2>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mnum1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mnum2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mindex_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morganized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morganized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morganized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0morganized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be 1-dimensional or an integer"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size=64):\n",
    "    # each sample has shape (2, 28, 28), so each batch_x has (batch_size, 2, 28, 28) and vatch_y has (batch_size)\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for _ in range(batch_size):\n",
    "        if np.random.random() < .5:\n",
    "            # match\n",
    "            num = (int)(np.random.random() * 10)\n",
    "            index_1,index_2 = np.random.randint(0, high=len(organized[num])),np.random.randint(0, high=len(organized[num]))\n",
    "            batch_x.append([organized[num][index_1],organized[num][index_2]])\n",
    "            batch_y.append(1)\n",
    "        else:\n",
    "            num1 = (int)(np.random.random() * 10)\n",
    "            num2 = np.random.choice(list(range(10)).remove(num1))\n",
    "            index_1,index_2 = np.random.randint(0, high=len(organized[num1])),np.random.randint(0, high=len(organized[num2]))\n",
    "            batch_x.append([organized[num1][index_1],organized[num2][index_2]])\n",
    "            batch_y.append(0)\n",
    "    return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "x,y = generate_batch(batch_size=10)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
