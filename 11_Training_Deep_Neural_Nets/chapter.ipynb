{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Training Deep Neural Nets\n",
    "\n",
    "The training of a small neural net like what we used in the MNIST or iris example datasets is trivial. They are quick to train and don't have too many parameters. However, in the case of more complex problems, you may need neural nets with much more complexity. This gives rise to 3 main issues:\n",
    "\n",
    "1. The problem of *vanishing gradients* (or the related *exploding gradients* problem) which will be explained in a bit.\n",
    "\n",
    "2. Training such a large neural net will take a long time.\n",
    "\n",
    "3. A model with millions or billions of parameters has a very high capability of overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients Problem\n",
    "\n",
    "Originally, neural nets were initialized with weights and biases sampled from a distribution of mean 0 and std 1. Researchers at the time, though, noticed that the process of backpropogation had issues that either the gradient would get smaller and smaller the deeper into the network it went and thus the first few layers would be left nearly unchanged after each step (meaning the network would take years to converge to a solution, if that), or alternatively, the gradients trended to infinity as backpropogation went through each layer and the algorithm would diverge.\n",
    "\n",
    "Researchers Xavier Glorot and Yoshua Bengio in 2010 discovered that on a large scale, for each of the layers, the variance of the outputs was much greater than the variance of its inputs. \"Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers.\"\n",
    "\n",
    "The derivative of the sigmoid/logistic function at high and low values is near 0, so as the variance keeps increasing, the activation functions \"saturate\" and thus when \"backpropagation kicks in, it has virtually no gradient to propagate back through the network, and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xavier's solution was to ensure that the variance of the input and output of each layer was approximately the same. This is not actually possible when the number of inputs != number of outputs, but the compromise that Xavier proposed was to chose weights from a normal distribution with mean 0 and std = sqrt(2/(n_inputs+n_outputs))\n",
    "\n",
    "This approach has been shown to speed up the training of neural nets significantly. Since then, different distributions for different activation functions besides sigmoid have been proposed, but Tensorflow will handle this all for you, so I won't bother writing them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "\n",
    "While we know that biological neurons use the sigmoid activation function, this 2010 paper suggests that in ANNs, the sigmoid function is not the best choice. If we use the ReLU instead, we know that it will never saturate (and also it helps that it is much faster to calculate too).\n",
    "\n",
    "One common issue with the ReLU activation function however is that neurons with ReLU often \"die\". If the weighted sum of all of a neuron's inputs is negative, the ReLU function will output 0. It's unlikely that its weights will ever update to fix this problem because its gradient will be 0, and thus the ReLU neuron will almost always output 0. It's possible that up to half of the neurons in a network will be \"dead\" if they all have the ReLU function.\n",
    "\n",
    "To solve this issue, we can use a variant of the ReLU function. One example is the *leaky ReLU* defined as leaky_relu(z,a)=max(az,z) where a<1. The book gives the analogy of the neuron \"going into a coma\" rather than dying with the leaky relu. While it has a small gradient when z<0, it will still turn back on given enough time. One paper suggests that setting a large leak a=0.2 outperforms when a=0.01, and definitely outperforms the original ReLU.\n",
    "\n",
    "Something else you can do is a *randomized leaky ReLU (RReLU)*, \"where a is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer.\" You could also have a be another parameter of the model to be trained through backpropagation in the case of *parametric leaky ReLU (PReLU)*, but for small datasets this could lead to some overfitting.\n",
    "\n",
    "However, a 2015 paper proposed another activation function that may be superior to all the others. Called the *exponential linear unit (ELU)* it is defined as elu(z,a) = (a(exp(z)-1) if z < 0, z if z>=0. The only drawback for this function it seems is that it is slower to compute than the other ReLU variants, but \"this is compensated by the faster convergence rate. However, at test time, an ELU network will be slower than a ReLU network.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.elu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.keras.activations.relu)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras doesn't come with any more of the activation fns that we just discussed, but it is\n",
    "# very easy to make your own like so:\n",
    "def leaky_relu(z,alpha=0.2):\n",
    "    return max(alpha*z,z)\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10,activation=leaky_relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "\"Although using He initialization along with ELU... can significantly reduce the vanishing/exploding gradients problems at the beginning of training, it doesn't guarantee that they won't come back.\"\n",
    "\n",
    "Another really powerful way to reduce the effect of vanishing/exploding gradients is to use Batch Normalization. The idea is that before the activation function is applied to the weighted sums in each layer. It first normalizes and zero-centers the values, and then it uses two additional parameters per layer to scale and then shift all the values.\n",
    "\n",
    "This technique is also used to solve what is called the **Internal Covariate Shift** problem. The normalization and zero-centering of the data addresses \"the problem that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BN algorithm first takes an empirical mean and an empirical std of the whole batch.\n",
    "\n",
    "Then, it zero-centers and normalizes each input by: *new_xi = (xi - mean)/sqrt(std^2 - epsilon)*. This should seem very familiar because it is exactly what I learned back in AP Stats, however there is that epsilon > 0 term added just to ensure there is no division by 0.\n",
    "\n",
    "Then, the algorithm scales the new values by **y** and shifts it by **beta**, i.e., *z = ***y***new_xi + ***beta***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"At test time, there is no mini-batch to compute the empirical mean and std, so instead you simply use the whole training set's mean and std.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
