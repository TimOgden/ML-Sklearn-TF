{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "\"SVMs are particularly well suited for classification of complex but small- or medium-sized datasets.\"\n",
    "\n",
    "The idea behind SVMs can be thought of as \"fitting the widest possible street... between the classes.\"\n",
    "\n",
    "If you train a linear classification model on two classes, it may accurately separate the classes in the training set, however it may come very close to the instances, so much that it may not be accurate when given new instances it hasn't trained on. The idea of fitting the widest street allows for the best separation between the classes.\n",
    "\n",
    "This process is called **large margin classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we add more training instances clearly towards one class or another, it will not change the slope or location of the road at all. This is because the decision boundary is \"fully determined (or \"supported\") by the instances located on the edge of the street. These instances are called **support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin Classification\n",
    "Strictly forcing all positive instances to be off the street and on the right side is called a **hard margin**. The issue with  this kind of classification is that it is super susceptible to outliers and that it will only work if the data is linearly seperable.\n",
    "\n",
    "A better solution is **soft margin classification**. Here, \"the objective is to find a good balance between keeping the street as large as possible and limiting the **margin violiations** (i.e., instances that end up in the middle of the street or even on the wrong side)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the balance of this by changing the 'C' hyperparameter of the SVM model in Scikit-learn. A small C value will have a larger margin with more margin violations compared to a large C value.\n",
    "\n",
    "A good regularization technique to reduce overfitting of SVMs is to just increase C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('linear_svc',\n",
       "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "x = iris['data'][:,(2,3)] # just pedal length and pedal width\n",
    "y = (iris['target'] == 2).astype(np.float64) # Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, SVM classifiers do not output probabilities for each class like a Logistic Regression does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear SVM Classification\n",
    "\n",
    "Most datasets you deal with will not be linearly separable. How do we deal with this?\n",
    "\n",
    "Well, just as we discussed in chapter 4, we can use the `PolynomialFeatures` class of Scikit-learn to make something linearly separable. If you have a bunch of instances with only one feature, it's likely they are not linearly separable along the x1 axis. However, if you define a second instance x2 = x1^2, then on the graph of x1 v x2, they may be linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('poly_features',\n",
       "                 PolynomialFeatures(degree=3, include_bias=True,\n",
       "                                    interaction_only=False, order='C')),\n",
       "                ('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C=10, loss='hinge'))\n",
    "])\n",
    "x, y = make_moons()\n",
    "polynomial_svm_clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The issue with polynomial features is that a small amount of features is likely to underfit, and a high number of degrees will result in a lot of features and a very slow model. Luckily, with SVMs, we can simply use a polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernels\n",
    "\n",
    "The 'kernel trick' is a \"miraculous mathematical technique... that makes it possible to get the same result as if you added many polynomial features, even with very high degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features since you don't actually add any features.\" Let's see this in action in Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Similarity Features\n",
    "\"Another technique to tackle nonlinear problems is to add features computed using a **similarity function** that measures how much each instance resembles a particular **landmark**.\" We can define a few instances to be a landmark, then choose a similarity function like the *Gaussian Radial Basis Function* to compare every instance to it.\n",
    "\n",
    "Then we can define a new feature for each landmark instance to each instance in the training set to be the evaluation of the similarity function when inputted the landmark and the instance. Lots of words here and I can't really add pictures so check the book if you need.\n",
    "\n",
    "Anyways, the new graph with more features is more likely to be linearly separable. It may be difficult to choose the landmarks, and the simplest solution is to use every instance as a landmark. \"The downside is that a training set with m instances and n features get transformed into a training set with m instances and m features (assumng you drop the original features). If your training set is very large, you end up with an equally large number of features.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian RBF Kernel\n",
    "\n",
    "Just how the polynomial kernel of the SVM solved our computational issues of the `PolynomialFeatures` class, we can use a kernel with a similarity function. \"It makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them. Let's try the Gaussian RBF kernel using the SVC class:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='rbf', gamma=5, C=0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma works as a regularization hyperparameter just like C. Increasing it will make the positive class boundary smaller, and decreasing it will make it wider. If your model is overfitting, try decreasing gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Complexity\n",
    "\n",
    "The `LinearSVC` model has an optimized algorithm specific for SVMs, but the algorithm cannot support the kernel trick. For the kernel trick, you need to use the `SVC` class. However, this class is really good at complex, but small to medium-sized datasets, but performs really slowly on large (e.g., hundreds of thousands of instances) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression\n",
    "\n",
    "Believe it or not, the SVM algorithm is also capable of doing regression problems. \"The trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible *on* the street... The width of the street is controlled by a hyperparameter epsilon.\n",
    "\n",
    "\"You can use Scikit-Learn's `LinearSVR` class to perform linear SVM Regression.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the kernel trick on SVM regression with the `SVR` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "    gamma='auto_deprecated', kernel='poly', max_iter=-1, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel='poly', degree=3, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
